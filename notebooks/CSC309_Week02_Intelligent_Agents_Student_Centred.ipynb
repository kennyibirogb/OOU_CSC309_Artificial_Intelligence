{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kennyibirogb/OOU_CSC309_Artificial_Intelligence/blob/main/notebooks/CSC309_Week02_Intelligent_Agents_Student_Centred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e430ba",
      "metadata": {
        "id": "40e430ba"
      },
      "source": [
        "# CSC309 ‚Äì Artificial Intelligence  \n",
        "**Week 2 Lab:** Intelligent Agents ‚Äî Random vs Reflex vs Model‚Äëbased\n",
        "\n",
        "**Instructor:** Dr Sakinat Folorunso\n",
        "\n",
        "**Title:** Associate Professor of AI Systems and FAIR Data **Department:** Computer Sciences, Olabisi Onabanjo University, Ago-Iwoye, Ogun State, Nigeria\n",
        "\n",
        "**Course Code:** CSC 309\n",
        "\n",
        "**Mode:** Student‚Äëcentred, hands‚Äëon in Google Colab\n",
        "\n",
        "> Every code cell is commented line‚Äëby‚Äëline so you can follow the logic precisely."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ffbb320",
      "metadata": {
        "id": "4ffbb320"
      },
      "source": [
        "## How to use this notebook\n",
        "1. Start with the **Group Log** and **Do Now**.  \n",
        "2. Run the **Setup** cell once.  \n",
        "3. Work through **Tasks**. Edit only cells marked **`# TODO(Student)`**.  \n",
        "4. Use **Quick Checks** to test your understanding.  \n",
        "5. Finish with the **Reflection**. If you finish early, try the **Extensions**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8f1b6855",
      "metadata": {
        "id": "8f1b6855",
        "outputId": "3d67ea8c-82dc-4d70-ddef-e9812bc44d4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üë• Group: IBIROGBA KEHINDE \n",
            "üìù Notes: Driver/Navigator, decisions, questions\n"
          ]
        }
      ],
      "source": [
        "#@title üßëüèΩ‚Äçü§ù‚Äçüßëüèæ Group Log (fill before you start)\n",
        "# The '#@param' annotations create form fields in Colab for easy input.\n",
        "\n",
        "group_members = \"IBIROGBA KEHINDE \"  #@param {type:\"string\"}  # Names of teammates\n",
        "roles_notes = \"Driver/Navigator, decisions, questions\"  #@param {type:\"string\"}  # Short working notes\n",
        "\n",
        "print(\"üë• Group:\", group_members)        # Echo the group list for confirmation\n",
        "print(\"üìù Notes:\", roles_notes)          # Echo the notes so they're preserved in output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40c3630",
      "metadata": {
        "id": "f40c3630"
      },
      "source": [
        "### Learning Objectives\n",
        "- Define **PEAS**, **rationality**, and **performance measures**.  \n",
        "- Implement a small environment and three agent policies.  \n",
        "- Compare policies using score distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "88155c41",
      "metadata": {
        "id": "88155c41",
        "outputId": "736bbc59-7ffd-4b89-98d2-7beb86c6f463",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Setup complete for Week 2.\n"
          ]
        }
      ],
      "source": [
        "#@title üîß Setup (run once)\n",
        "# This lab uses only common scientific Python libraries.\n",
        "# Each import line is commented to explain its role.\n",
        "\n",
        "import sys                  # Access to Python interpreter details (not strictly required)\n",
        "import subprocess           # Allows us to call 'pip' if needed\n",
        "def pip_install(pkgs):      # Helper to install packages only if missing\n",
        "    for p in pkgs:\n",
        "        try:\n",
        "            __import__(p.split(\"==\")[0])   # Try to import the package by name\n",
        "        except Exception:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", p])  # Quiet install\n",
        "\n",
        "pip_install([\"numpy\", \"matplotlib\"])       # We need NumPy for arrays and Matplotlib for plots\n",
        "\n",
        "import numpy as np           # Numerical arrays and random sampling\n",
        "import random                # Simple random choices for agent actions\n",
        "import matplotlib.pyplot as plt  # Basic plotting for histograms\n",
        "\n",
        "print(\"‚úÖ Setup complete for Week 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0yiWwzYPfCdP"
      },
      "id": "0yiWwzYPfCdP"
    },
    {
      "cell_type": "markdown",
      "id": "e641aa87",
      "metadata": {
        "id": "e641aa87"
      },
      "source": [
        "### Do Now\n",
        "Sketch a quick **PEAS** for a campus cleaning robot (Performance, Environment, Actuators, Sensors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484faeee",
      "metadata": {
        "id": "484faeee"
      },
      "outputs": [],
      "source": [
        "#@title üß™ Environment + Policies (fully commented)\n",
        "# We implement a tiny \"vacuum-world\" style grid with dirt.\n",
        "# The agent gets +10 for cleaning a dirty cell and ‚àí1 for moving or cleaning a clean cell.\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, n=5, dirt_prob=0.3, seed=0):\n",
        "        random.seed(seed)                      # Fix Python's random seed for reproducibility\n",
        "        np.random.seed(seed)                   # Fix NumPy's random seed for reproducibility\n",
        "        self.n = n                             # Grid size (n x n)\n",
        "        self.agent_pos = (0, 0)                # Start position in the top‚Äëleft corner\n",
        "        self.dirt = (np.random.rand(n, n) < dirt_prob).astype(int)  # 1 indicates dirt; 0 is clean\n",
        "        self.score = 0                         # Cumulative score earned by the agent\n",
        "\n",
        "    def perceive(self):\n",
        "        x, y = self.agent_pos                  # Unpack the current coordinates\n",
        "        return {\"dirty\": bool(self.dirt[x, y])}# Observation: is the current cell dirty?\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_pos                  # Current position of the agent\n",
        "        if action == \"CLEAN\":                  # If the agent chooses to clean\n",
        "            if self.dirt[x, y] == 1:          # Check if the current cell actually has dirt\n",
        "                self.dirt[x, y] = 0           # Remove the dirt\n",
        "                self.score += 10              # Reward for cleaning dirt\n",
        "            else:\n",
        "                self.score -= 1               # Penalty for cleaning when there is no dirt\n",
        "        elif action in [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]:  # If the agent chooses to move\n",
        "            nx, ny = x, y                     # Start with the current position\n",
        "            if action == \"UP\":   nx = max(0, x - 1)              # Move up, staying inside the grid\n",
        "            if action == \"DOWN\": nx = min(self.n - 1, x + 1)     # Move down, staying inside the grid\n",
        "            if action == \"LEFT\": ny = max(0, y - 1)              # Move left, staying inside the grid\n",
        "            if action == \"RIGHT\":ny = min(self.n - 1, y + 1)     # Move right, staying inside the grid\n",
        "            self.agent_pos = (nx, ny)           # Update the agent's position\n",
        "            self.score -= 1                     # Small movement penalty\n",
        "        else:\n",
        "            self.score -= 1                     # Penalize unknown actions to keep policy sensible\n",
        "        return self.perceive()                  # Return the new observation\n",
        "\n",
        "# --- Policies ---------------------------------------------------------------\n",
        "\n",
        "def random_agent(obs):\n",
        "    \"\"\"Return a random action, ignoring the observation (baseline).\"\"\"\n",
        "    return random.choice([\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"CLEAN\"])  # Uniform random choice\n",
        "\n",
        "def reflex_agent(obs):\n",
        "    \"\"\"Clean if dirty; otherwise move randomly (simple reflex).\"\"\"\n",
        "    if obs[\"dirty\"]:                      # If the sensor says current cell is dirty\n",
        "        return \"CLEAN\"                    # Then clean it\n",
        "    return random.choice([\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"])  # Else move randomly\n",
        "\n",
        "def run(agent_fn, steps=100, seed=0):\n",
        "    \"\"\"Simulate an agent for a fixed number of steps and return the final score.\"\"\"\n",
        "    env = GridWorld(seed=seed)            # Create a fresh environment per run\n",
        "    for _ in range(steps):                # Repeat for the given number of steps\n",
        "        obs = env.perceive()              # Read the current observation\n",
        "        action = agent_fn(obs)            # Choose an action using the policy\n",
        "        env.step(action)                  # Apply the action to the environment\n",
        "    return env.score                      # Return total score as performance measure\n",
        "\n",
        "# Quick experiment: average scores over 5 seeds for the two base policies\n",
        "for fn in [random_agent, reflex_agent]:             # Iterate over the two policy functions\n",
        "    scores = [run(fn, seed=s) for s in range(5)]    # Run each policy with seeds 0..4\n",
        "    print(fn.__name__, \"avg score:\", sum(scores)/len(scores))  # Print the average score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c0bb22d",
      "metadata": {
        "id": "2c0bb22d"
      },
      "outputs": [],
      "source": [
        "# TODO(Student): Model‚Äëbased agent + comparison plot (with line‚Äëby‚Äëline comments)\n",
        "\n",
        "def model_based_agent_factory():\n",
        "    \"\"\"Return an agent function that remembers visited cells (very simple model).\"\"\"\n",
        "    visited = set()                                   # 'visited' will store coordinates seen before\n",
        "    last_move = [\"RIGHT\", \"DOWN\", \"LEFT\", \"UP\"]       # A simple move preference order\n",
        "\n",
        "    def agent(obs, _cache={\"pos\": (0, 0)}):\n",
        "        # The '_cache' dict stores the last known position; this keeps state between calls.\n",
        "        # In this simple demo we will not compute the position here (GridWorld hides it),\n",
        "        # but we can still use 'visited' as a proxy memory if we had access to coordinates.\n",
        "        if obs.get(\"dirty\"):                          # If current cell is dirty\n",
        "            return \"CLEAN\"                            # Always clean first\n",
        "        # Otherwise, just cycle through moves to explore the grid deterministically.\n",
        "        move = last_move[0]                           # Take the first preferred move\n",
        "        last_move.append(last_move.pop(0))            # Rotate the preference list\n",
        "        return move                                   # Return the chosen move\n",
        "    return agent                                      # Return the stateful agent function\n",
        "\n",
        "# --- Evaluation helper (fully commented) ------------------------------------\n",
        "def evaluate(agent_fn, trials=30):\n",
        "    \"\"\"Run the given agent across many random seeds and collect scores.\"\"\"\n",
        "    results = []                                      # Empty list to hold the scores\n",
        "    for i in range(trials):                           # Loop over 'trials' seeds\n",
        "        score = run(agent_fn, seed=i)                 # Run one simulation with seed 'i'\n",
        "        results.append(score)                         # Append the score to the results list\n",
        "    return results                                    # Return the list of scores\n",
        "\n",
        "# Compute scores for all three policies\n",
        "random_scores = evaluate(random_agent)                # List of 30 scores for the random policy\n",
        "reflex_scores = evaluate(reflex_agent)                # List of 30 scores for the reflex policy\n",
        "model_scores = evaluate(model_based_agent_factory())  # List of 30 scores for the model-based policy\n",
        "\n",
        "# Plot histograms to compare score distributions\n",
        "plt.figure()                                         # Create a new figure\n",
        "plt.hist(random_scores, alpha=0.5, label=\"random\")   # Histogram for random agent\n",
        "plt.hist(reflex_scores, alpha=0.5, label=\"reflex\")   # Histogram for reflex agent\n",
        "plt.hist(model_scores, alpha=0.5, label=\"model-based\")# Histogram for model-based agent\n",
        "plt.legend()                                         # Show legend with labels\n",
        "plt.xlabel(\"score\")                                  # X-axis label\n",
        "plt.ylabel(\"frequency\")                              # Y-axis label\n",
        "plt.title(\"Agent score distribution (higher is better)\")  # Plot title\n",
        "plt.show()                                           # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e665821",
      "metadata": {
        "id": "7e665821"
      },
      "source": [
        "### Reflection\n",
        "- What **performance measure** did we implicitly design with our scoring?  \n",
        "- Which policy is most **rational** under this measure? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implictly designed a performanve measure that maximizes total accimulated score, meaning the ahent is rewarded for actions thag lead to the highest overall points.\n",
        "\n",
        "\n",
        "The most rational policy is the one that chooses actions that yoeld the highest total score over time , because the policy directlt aligns with maximizing the performance measure we defined.\n"
      ],
      "metadata": {
        "id": "J9c04APyfEag"
      },
      "id": "J9c04APyfEag"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}