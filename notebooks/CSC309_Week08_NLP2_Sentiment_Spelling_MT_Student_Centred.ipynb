{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakinat-Folorunso/OOU_CSC309_Artificial_Intelligence/blob/main/notebooks/CSC309_Week08_NLP2_Sentiment_Spelling_MT_Student_Centred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab61d1ea",
      "metadata": {
        "id": "ab61d1ea"
      },
      "source": [
        "# CSC309 ‚Äì Artificial Intelligence  \n",
        "**Week 8 Lab:** NLP II ‚Äî Sentiment, Spelling, (Optional) Translation & Speech\n",
        "\n",
        "**Instructor:** Dr Sakinat Folorunso  \n",
        "\n",
        "**Title:** Associate Professor of AI Systems and FAIR Data **Department:** Computer Sciences, Olabisi Onabanjo University, Ago-Iwoye, Ogun State, Nigeria\n",
        "\n",
        "**Course Code**: CSC 309\n",
        "\n",
        "**Mode:** Student‚Äëcentred, hands‚Äëon in Google Colab\n",
        "\n",
        "> Every code cell is commented line‚Äëby‚Äëline so you can follow the logic precisely."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d5f2258",
      "metadata": {
        "id": "2d5f2258"
      },
      "source": [
        "## How to use this notebook\n",
        "1. Start with the **Group Log** and **Do Now**.  \n",
        "2. Run the **Setup** cell once.  \n",
        "3. Work through **Tasks**. Edit only cells marked **`# TODO(Student)`**.  \n",
        "4. Use **Quick Checks** to test your understanding.  \n",
        "5. Finish with the **Reflection**. If you finish early, try the **Extensions**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119344d6",
      "metadata": {
        "id": "119344d6"
      },
      "outputs": [],
      "source": [
        "#@title üßëüèΩ‚Äçü§ù‚Äçüßëüèæ Group Log (fill before you start)\n",
        "# The '#@param' annotations create form fields in Colab for easy input.\n",
        "\n",
        "group_members = \"Type names here\"  #@param {type:\"string\"}  # Names of teammates\n",
        "roles_notes = \"Driver/Navigator, decisions, questions\"  #@param {type:\"string\"}  # Short working notes\n",
        "\n",
        "print(\"üë• Group:\", group_members)        # Echo the group list for confirmation\n",
        "print(\"üìù Notes:\", roles_notes)          # Echo the notes so they're preserved in output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98999ada",
      "metadata": {
        "id": "98999ada"
      },
      "source": [
        "### Learning Objectives\n",
        "- Compare **rule‚Äëbased** vs. **ML** sentiment analysis.  \n",
        "- Implement **spelling correction**; optionally try MT/Speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18f9aa6c",
      "metadata": {
        "id": "18f9aa6c"
      },
      "outputs": [],
      "source": [
        "#@title üîß Setup\n",
        "# This lab uses NLTK for sentiment resources and scikit-learn for a small ML model.\n",
        "\n",
        "import sys, subprocess                                              # For optional installs\n",
        "def pip_install(pkgs):\n",
        "    for p in pkgs:\n",
        "        try: __import__(p.split(\"==\")[0])                           # Try importing\n",
        "        except Exception:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", p])  # Install if missing\n",
        "pip_install([\"nltk\", \"scikit-learn\", \"pyspellchecker\"])             # Required packages\n",
        "\n",
        "import nltk                                                         # NLP toolkit (tokenizers, corpora)\n",
        "try: nltk.data.find('sentiment/vader_lexicon')                      # Try to find VADER lexicon locally\n",
        "except LookupError: nltk.download('vader_lexicon')                  # If missing, download it\n",
        "try: nltk.data.find('corpora/movie_reviews')                        # Movie reviews for ML demo\n",
        "except LookupError: nltk.download('movie_reviews')\n",
        "try: nltk.data.find('tokenizers/punkt')                             # Tokenizer (may be needed)\n",
        "except LookupError: nltk.download('punkt')\n",
        "\n",
        "print(\"‚úÖ Setup complete for Week 8.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948614ee",
      "metadata": {
        "id": "948614ee"
      },
      "outputs": [],
      "source": [
        "#@title üôÇ Sentiment: VADER (rule‚Äëbased) and LinearSVC (ML) ‚Äî fully commented\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer               # VADER sentiment analyzer\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()                                  # Create a VADER instance\n",
        "print(\"VADER demo:\", sia.polarity_scores(\"I absolutely love this course!\"))  # Show a quick polarity dict\n",
        "\n",
        "# --- ML sentiment on movie_reviews -----------------------------------------\n",
        "from nltk.corpus import movie_reviews                               # Access labeled movie review data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer         # Turn text into TF‚ÄëIDF features\n",
        "from sklearn.svm import LinearSVC                                   # Linear SVM classifier\n",
        "from sklearn.model_selection import train_test_split                # Split into train/test\n",
        "\n",
        "# Join tokens into raw text strings for each review\n",
        "docs = [\" \".join(movie_reviews.words(fid)) for fid in movie_reviews.fileids()]  # List of review texts\n",
        "labels = [movie_reviews.categories(fid)[0] for fid in movie_reviews.fileids()]  # 'pos' or 'neg' labels\n",
        "\n",
        "Xtr_raw, Xte_raw, ytr, yte = train_test_split(docs, labels, test_size=0.2, random_state=42)  # Train/test split\n",
        "vec = TfidfVectorizer(min_df=3)                                       # Ignore very rare terms\n",
        "Xtr = vec.fit_transform(Xtr_raw)                                      # Fit vectorizer on training text\n",
        "Xte = vec.transform(Xte_raw)                                          # Transform test text with same vocab\n",
        "clf = LinearSVC()                                                     # Instantiate a linear SVM\n",
        "clf.fit(Xtr, ytr)                                                     # Train the classifier\n",
        "print(\"Movie review accuracy:\", clf.score(Xte, yte))                  # Report accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d4d679",
      "metadata": {
        "id": "47d4d679"
      },
      "outputs": [],
      "source": [
        "#@title ‚úÖ Spelling correction (pyspellchecker) ‚Äî fully commented\n",
        "from spellchecker import SpellChecker                                 # Import the SpellChecker class\n",
        "spell = SpellChecker()                                                # Create a spell checker instance\n",
        "\n",
        "sentence = \"Ths is a sentnce with som misspelled wrds\"               # Example sentence with typos\n",
        "tokens = sentence.split()                                            # Tokenize by simple whitespace split\n",
        "misspelled = spell.unknown(tokens)                                   # Identify words not in dictionary\n",
        "corrections = {w: spell.correction(w) for w in misspelled}           # Map each misspelled word to its best guess\n",
        "print(\"Original:\", sentence)                                         # Show the original sentence\n",
        "print(\"Corrections:\", corrections)                                   # Show the suggested corrections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a61680",
      "metadata": {
        "id": "f3a61680"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Translation with MarianMT (large download in Colab)\n",
        "# !pip -q install transformers sentencepiece                         # Install only if you want to try\n",
        "# from transformers import MarianMTModel, MarianTokenizer            # Import translation model/tokenizer\n",
        "# model_name = \"Helsinki-NLP/opus-mt-en-fr\"                          # Choose an English‚ÜíFrench model\n",
        "# tok = MarianTokenizer.from_pretrained(model_name)                  # Load tokenizer\n",
        "# model = MarianMTModel.from_pretrained(model_name)                  # Load model\n",
        "# src = [\"Artificial Intelligence is exciting.\"]                     # Source sentence list\n",
        "# out = model.generate(**tok(src, return_tensors=\"pt\", padding=True))# Generate translations\n",
        "# print(tok.batch_decode(out, skip_special_tokens=True))             # Decode to strings"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}